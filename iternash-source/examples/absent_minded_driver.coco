from math import log

from scipy.special import comb
from mpmath import hyp2f1

from iternash import (
    Game,
    agent,
    expr_agent,
    bbopt_agent,
    debug_agent,
    human_agent,
)
from iternash.util import clip


common_params = dict(
    # necessary number of defections
    d=2,
    # number of deployment episodes
    m=100,
    # desired PC for the n agent to attempt to achieve
    eps=0.01,
    # probability of training defection leading to no reward for that agent
    p_mod=0.9,
    # utility for defecting during training
    r_n=0,
    # utility for defecting during deployment
    r_m=1,
    # utility for never defecting
    r_f=0,
)


# optimal training episodes in the one defection game
baseline_n_agent = expr_agent(
    name="n",
    expr="m/p_mod * (1-eps)/eps",
    default=common_params["m"],
)


# optimal defection probability in the one defection game
baseline_p_agent = expr_agent(
    name="p",
    expr="""(
        (n * p_mod)
        / (n * p_mod + m)
        * (r_m - r_n)/(r_m - r_f)
    )**(1/(m-1))""",
    default=0.9,
)


# probability of catastrophe in the one defection game
baseline_PC_agent = expr_agent(
    name="PC",
    expr="1 - p**m",
    default=0.1,
)


# generic expected reward calculator
ER_agent = expr_agent(
    name="ER",
    expr="""
    (1 - (p + (1-p)*(1-p_mod))**n) * r_n
    + (p + (1-p)*(1-p_mod))**n * (
        PC * r_m
        + (1-PC) * r_f
    )
    """,
)


# probability of catastrophe in the non-sequential two defection game
nonseq_2d_PC_agent = expr_agent(
    name="PC",
    expr="1 - p**m - m*(1-p)*p**(m-1)",
    default=0.1,
)

@agent(name="PC", default=0.1)
def nonseq_d_PC_agent(env) =
    d = env["d"]
    if d == 2:
        return nonseq_2d_PC_agent(env)
    comb(m, d) * p**(m-d) * (1-p)**d * (float..hyp2f1)(1, d-m, d+1, (p-1)/p) where:
        m = env["m"]
        p = env["p"]


# probability of catastrophe in the sequential defection game
@agent(name="PC", default=0.1)
def _exact_seq_d_PC_agent(env):
    m = env["m"]
    d = env["d"]
    PC = 0
    for i in range(1, int(m/d) + 1):
        PC += expr_agent(None, """
        (-1)**(i+1) * (1-p + p*(m-i*d+1)/i) * comb(m-i*d, i-1) * (1-p)**(i*d) * p**(i-1)
        """, vars={"i": i, "comb": comb})(env)
    return PC

seq_d_PC_agent = expr_agent(
    name="PC",
    expr="(1-p)**(d-1) * (1 - p**(m-d+1))",
    default=0.1,
)


# black-box-optimized p agent
bbopt_p_agent = bbopt_agent(
    name="p",
    tunable_actor=(bb, env) -> 1 - bb.loguniform("p", 0.000001, 1),
    util_func=env -> log(clip(env["ER"])),
    file=__file__,
    default=0.9,
)


# black-box-optimized n agent that attempts to set PC to eps
bbopt_n_agent = bbopt_agent(
    name="n",
    tunable_actor=(bb, env) ->
        int(baseline_n_agent(env) * bb.loguniform("n/n_c", 0.001, 1000)),
    util_func=expr_agent(None, "-abs(log(PC) - log(eps))", vars={"log": log}),
    file=__file__,
    default=common_params["m"],
)


# agent that prints n, p, PC every 100 steps
periodic_debugger = debug_agent("n = {n}; p = {p}; PC = {PC}; ER = {ER}", period=100)


# absent-minded driver game where catastrophe occurs on the first defection
baseline_game = Game(
    "baseline",
    baseline_n_agent,
    baseline_p_agent,
    baseline_PC_agent,
    ER_agent,
    default_run_steps=1,
    **common_params,
)


# absent-minded driver game where catastrophe occurs upon the
#  second defection during deployment with a conservative n
conservative_nonseq_d_game = Game(
    "conservative_nonseq_d",
    baseline_n_agent,
    bbopt_p_agent,
    nonseq_d_PC_agent,
    ER_agent,
    periodic_debugger,
    default_run_steps=500,
    **common_params,
)


# absent-minded driver game where catastrophe occurs if there are ever
#  d sequential defections during deployment with a conservative n
#  and p approximated by BBopt
conservative_seq_d_game = Game(
    "conservative_seq_d",
    baseline_n_agent,
    bbopt_p_agent,
    seq_d_PC_agent,
    ER_agent,
    periodic_debugger,
    default_run_steps=500,
    **common_params,
)


# game for testing the impact of different p values in the non-sequential
#  vs. sequential cases
test_game = Game(
    "test",
    baseline_n_agent,
    human_agent(name="p"),
    baseline_PC=baseline_PC_agent,
    baseline_ER=(def env ->
        new_env = env.copy();
        new_env["PC"] = env["baseline_PC"];
        ER_agent(new_env)),
    nonseq_d_PC=nonseq_d_PC_agent,
    nonseq_d_ER=(def env ->
        new_env = env.copy();
        new_env["PC"] = env["nonseq_d_PC"];
        ER_agent(new_env)),
    seq_d_PC=seq_d_PC_agent,
    seq_d_ER=(def env ->
        new_env = env.copy();
        new_env["PC"] = env["seq_d_PC"];
        ER_agent(new_env)),
    default_run_steps=100,
    **common_params,
)


if __name__ == "__main__":
    print("\nRunning baseline game...")
    baseline_game.run() |> print

    print("\nRunning conservative non-sequential two defection game...")
    conservative_nonseq_d_game.run() |> print

    print(f"\nRunning conservative sequential defection game with d = {conservative_seq_d_game.env['d']}...")
    conservative_seq_d_game.run() |> print

    print("\nRunning test game...")
    test_game.run() |> print
